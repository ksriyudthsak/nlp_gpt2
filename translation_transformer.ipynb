{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This jupyter-notebook is for my study. Codes was modified based on the following references\n",
    "# # https://pypi.org/project/keras-transformer/\n",
    "# https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# make only specific GPU to be utilized\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# select GPU to run on\n",
    "GPU = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU\n",
    "\n",
    "# stop GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"-1\"\n",
    "\n",
    "# set GPU to be deterministic \n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\" # set hash environment\n",
    "os.environ[\"TF_CUDNN_USE_AUTOTUNE\"] = \"0\" # use cuDNN function to retrieve the best algorithm\n",
    "os.environ[\"TF_CUDNN_CONVOLUTION_BWD_FILTER_ALGO_DETERMINISTIC\"]='1' # use cuDNN deterministic algorithms\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1' # deterministic setting\n",
    "\n",
    "# set numpy, python, tensorflow random seed\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import random\n",
    "random.seed(10)\n",
    "seed(10)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(20)\n",
    "\n",
    "# GPU memory control\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.40\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset was download from http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total line 45449\n"
     ]
    }
   ],
   "source": [
    "# Set up path to the data textfile\n",
    "data_path = 'data/jpn-eng/jpn.txt'\n",
    "# data_path = 'data/tha-eng/tha.txt'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "print (\"total line\", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters for training\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 200  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "start_samples1 = 0\n",
    "end_samples1 = 5000 \n",
    "start_samples2 = 40000\n",
    "end_samples2 = 45000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    " \n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    " \n",
    "# Generate dictionaries\n",
    "def build_token_dict(token_list):\n",
    "    token_dict = {\n",
    "        '<PAD>': 0,\n",
    "        '<START>': 1,\n",
    "        '<END>': 2,\n",
    "    }\n",
    "    for tokens in token_list:\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = len(token_dict)\n",
    "    return token_dict\n",
    "\n",
    "source_input = []\n",
    "source_tokens = []\n",
    "target_tokens = []\n",
    "\n",
    "for line in lines[start_samples1: min(end_samples1, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')       \n",
    "    source_input.append(input_text)\n",
    "    source_tokens.append(input_text.split(' '))\n",
    "    target_tokens.append(target_text.split(' '))\n",
    "\n",
    "for line in lines[start_samples2: min(end_samples2, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')       \n",
    "    source_input.append(input_text)\n",
    "    source_tokens.append(input_text.split(' '))\n",
    "    target_tokens.append(target_text.split(' '))\n",
    "    \n",
    "source_token_dict = build_token_dict(source_tokens)\n",
    "target_token_dict = build_token_dict(target_tokens)\n",
    "source_token_dict_inv = {v: k for k, v in source_token_dict.items()}\n",
    "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}\n",
    "\n",
    "\n",
    "# Add special tokens\n",
    "encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "\n",
    "# Padding\n",
    "source_max_len = max(map(len, encode_tokens))\n",
    "target_max_len = max(map(len, decode_tokens))\n",
    "\n",
    "encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
    "output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n",
    "encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
    "decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Token-Embedding (Embedd [(None, None, 32), ( 278016      Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Embedding (TrigPosEmbed (None, None, 32)     0           Encoder-Token-Embedding[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     4224        Encoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-Embedding[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     64          Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 32)     8352        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 32)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 32)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 32)     64          Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     4224        Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Token-Embedding (Embedd [(None, None, 32), ( 278016      Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 32)     0           Decoder-Token-Embedding[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     64          Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     4224        Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, None, 32)     8352        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, None, 32)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, None, 32)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     64          Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, None, 32)     64          Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     4224        Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     64          Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 32)     8352        Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 32)     0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 32)     0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 32)     64          Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     4224        Decoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     64          Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     4224        Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     64          Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward (FeedForw (None, None, 32)     8352        Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Dropout ( (None, None, 32)     0           Decoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Add (Add) (None, None, 32)     0           Decoder-2-MultiHeadQueryAttention\n",
      "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Norm (Lay (None, None, 32)     64          Decoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 8688)   8688        Decoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-Token-Embedding[0][1]    \n",
      "==================================================================================================\n",
      "Total params: 624,112\n",
      "Trainable params: 624,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras_transformer import get_model\n",
    "from keras import optimizers\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "    embed_dim=32,\n",
    "    encoder_num=2,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=128,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=False,  # Use different embeddings for different languages\n",
    ")\n",
    "\n",
    "# adam_opt = optimizers.Adam(lr=0.0005)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1222 10:41:41.822609 139836474173248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1222 10:41:48.488249 139836474173248 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 2.0931 - accuracy: 0.5981 - val_loss: 1.3244 - val_accuracy: 0.6651\n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.3148 - accuracy: 0.6661 - val_loss: 1.4172 - val_accuracy: 0.6654\n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.2832 - accuracy: 0.6663 - val_loss: 1.3849 - val_accuracy: 0.6658\n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.2527 - accuracy: 0.6666 - val_loss: 1.4812 - val_accuracy: 0.6663\n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.2222 - accuracy: 0.6664 - val_loss: 1.5636 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.1901 - accuracy: 0.6665 - val_loss: 1.5623 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.1581 - accuracy: 0.6666 - val_loss: 1.5896 - val_accuracy: 0.6658\n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.1202 - accuracy: 0.6671 - val_loss: 1.5630 - val_accuracy: 0.6661\n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.0746 - accuracy: 0.6675 - val_loss: 1.6128 - val_accuracy: 0.6659\n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.0227 - accuracy: 0.6689 - val_loss: 1.6172 - val_accuracy: 0.6661\n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.9634 - accuracy: 0.6718 - val_loss: 1.6224 - val_accuracy: 0.6658\n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.9016 - accuracy: 0.6758 - val_loss: 1.6962 - val_accuracy: 0.6659\n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.8341 - accuracy: 0.6815 - val_loss: 1.7656 - val_accuracy: 0.6664\n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.7665 - accuracy: 0.6912 - val_loss: 1.7878 - val_accuracy: 0.6663\n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.6951 - accuracy: 0.7033 - val_loss: 1.8824 - val_accuracy: 0.6666\n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.6240 - accuracy: 0.7186 - val_loss: 1.8898 - val_accuracy: 0.6671\n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5541 - accuracy: 0.7387 - val_loss: 1.9813 - val_accuracy: 0.6674\n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4886 - accuracy: 0.7656 - val_loss: 2.0515 - val_accuracy: 0.6676\n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4247 - accuracy: 0.7991 - val_loss: 2.1136 - val_accuracy: 0.6688\n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3672 - accuracy: 0.8299 - val_loss: 2.1841 - val_accuracy: 0.6683\n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3148 - accuracy: 0.8539 - val_loss: 2.2093 - val_accuracy: 0.6689\n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2681 - accuracy: 0.8739 - val_loss: 2.2212 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2279 - accuracy: 0.8855 - val_loss: 2.2187 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1965 - accuracy: 0.8949 - val_loss: 2.2221 - val_accuracy: 0.6694\n",
      "Epoch 25/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1722 - accuracy: 0.9005 - val_loss: 2.2328 - val_accuracy: 0.6688\n",
      "Epoch 26/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1543 - accuracy: 0.9027 - val_loss: 2.2448 - val_accuracy: 0.6691\n",
      "Epoch 27/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1402 - accuracy: 0.9059 - val_loss: 2.2404 - val_accuracy: 0.6693\n",
      "Epoch 28/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1331 - accuracy: 0.9064 - val_loss: 2.2429 - val_accuracy: 0.6698\n",
      "Epoch 29/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1254 - accuracy: 0.9075 - val_loss: 2.2452 - val_accuracy: 0.6693\n",
      "Epoch 30/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1216 - accuracy: 0.9059 - val_loss: 2.2481 - val_accuracy: 0.6691\n",
      "Epoch 31/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1165 - accuracy: 0.9094 - val_loss: 2.2489 - val_accuracy: 0.6691\n",
      "Epoch 32/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1139 - accuracy: 0.9100 - val_loss: 2.2480 - val_accuracy: 0.6693\n",
      "Epoch 33/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1125 - accuracy: 0.9082 - val_loss: 2.2478 - val_accuracy: 0.6696\n",
      "Epoch 34/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1097 - accuracy: 0.9087 - val_loss: 2.2477 - val_accuracy: 0.6694\n",
      "Epoch 35/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1067 - accuracy: 0.9100 - val_loss: 2.2500 - val_accuracy: 0.6691\n",
      "Epoch 36/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1056 - accuracy: 0.9112 - val_loss: 2.2447 - val_accuracy: 0.6691\n",
      "Epoch 37/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1049 - accuracy: 0.9118 - val_loss: 2.2497 - val_accuracy: 0.6693\n",
      "Epoch 38/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1047 - accuracy: 0.9117 - val_loss: 2.2412 - val_accuracy: 0.6693\n",
      "Epoch 39/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1035 - accuracy: 0.9102 - val_loss: 2.2505 - val_accuracy: 0.6691\n",
      "Epoch 40/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1020 - accuracy: 0.9117 - val_loss: 2.2486 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1025 - accuracy: 0.9101 - val_loss: 2.2472 - val_accuracy: 0.6691\n",
      "Epoch 42/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1011 - accuracy: 0.9107 - val_loss: 2.2490 - val_accuracy: 0.6696\n",
      "Epoch 43/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1001 - accuracy: 0.9114 - val_loss: 2.2512 - val_accuracy: 0.6693\n",
      "Epoch 44/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1002 - accuracy: 0.9112 - val_loss: 2.2488 - val_accuracy: 0.6693\n",
      "Epoch 45/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0989 - accuracy: 0.9121 - val_loss: 2.2509 - val_accuracy: 0.6701\n",
      "Epoch 46/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0990 - accuracy: 0.9117 - val_loss: 2.2521 - val_accuracy: 0.6688\n",
      "Epoch 47/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0984 - accuracy: 0.9102 - val_loss: 2.2489 - val_accuracy: 0.6696\n",
      "Epoch 48/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0973 - accuracy: 0.9117 - val_loss: 2.2481 - val_accuracy: 0.6699\n",
      "Epoch 49/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0966 - accuracy: 0.9112 - val_loss: 2.2516 - val_accuracy: 0.6693\n",
      "Epoch 50/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0965 - accuracy: 0.9127 - val_loss: 2.2501 - val_accuracy: 0.6699\n",
      "Epoch 51/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0975 - accuracy: 0.9107 - val_loss: 2.2474 - val_accuracy: 0.6689\n",
      "Epoch 52/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0958 - accuracy: 0.9107 - val_loss: 2.2491 - val_accuracy: 0.6696\n",
      "Epoch 53/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0941 - accuracy: 0.9121 - val_loss: 2.2508 - val_accuracy: 0.6698\n",
      "Epoch 54/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0941 - accuracy: 0.9114 - val_loss: 2.2512 - val_accuracy: 0.6693\n",
      "Epoch 55/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0943 - accuracy: 0.9120 - val_loss: 2.2527 - val_accuracy: 0.6698\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0931 - accuracy: 0.9118 - val_loss: 2.2490 - val_accuracy: 0.6698\n",
      "Epoch 57/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0929 - accuracy: 0.9112 - val_loss: 2.2506 - val_accuracy: 0.6689\n",
      "Epoch 58/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0942 - accuracy: 0.9110 - val_loss: 2.2510 - val_accuracy: 0.6694\n",
      "Epoch 59/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0929 - accuracy: 0.9108 - val_loss: 2.2490 - val_accuracy: 0.6694\n",
      "Epoch 60/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0921 - accuracy: 0.9125 - val_loss: 2.2503 - val_accuracy: 0.6696\n",
      "Epoch 61/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0908 - accuracy: 0.9127 - val_loss: 2.2513 - val_accuracy: 0.6694\n",
      "Epoch 62/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0926 - accuracy: 0.9111 - val_loss: 2.2469 - val_accuracy: 0.6698\n",
      "Epoch 63/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0903 - accuracy: 0.9132 - val_loss: 2.2499 - val_accuracy: 0.6696\n",
      "Epoch 64/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0910 - accuracy: 0.9121 - val_loss: 2.2512 - val_accuracy: 0.6699\n",
      "Epoch 65/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0904 - accuracy: 0.9119 - val_loss: 2.2499 - val_accuracy: 0.6696\n",
      "Epoch 66/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0892 - accuracy: 0.9124 - val_loss: 2.2490 - val_accuracy: 0.6701\n",
      "Epoch 67/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0902 - accuracy: 0.9121 - val_loss: 2.2470 - val_accuracy: 0.6704\n",
      "Epoch 68/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0896 - accuracy: 0.9096 - val_loss: 2.2501 - val_accuracy: 0.6701\n",
      "Epoch 69/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0895 - accuracy: 0.9120 - val_loss: 2.2496 - val_accuracy: 0.6699\n",
      "Epoch 70/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0896 - accuracy: 0.9122 - val_loss: 2.2488 - val_accuracy: 0.6694\n",
      "Epoch 71/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0885 - accuracy: 0.9136 - val_loss: 2.2517 - val_accuracy: 0.6693\n",
      "Epoch 72/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0887 - accuracy: 0.9120 - val_loss: 2.2481 - val_accuracy: 0.6698\n",
      "Epoch 73/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0884 - accuracy: 0.9111 - val_loss: 2.2476 - val_accuracy: 0.6699\n",
      "Epoch 74/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0875 - accuracy: 0.9125 - val_loss: 2.2482 - val_accuracy: 0.6699\n",
      "Epoch 75/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0876 - accuracy: 0.9129 - val_loss: 2.2475 - val_accuracy: 0.6703\n",
      "Epoch 76/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0859 - accuracy: 0.9130 - val_loss: 2.2483 - val_accuracy: 0.6694\n",
      "Epoch 77/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0866 - accuracy: 0.9115 - val_loss: 2.2483 - val_accuracy: 0.6698\n",
      "Epoch 78/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0868 - accuracy: 0.9124 - val_loss: 2.2470 - val_accuracy: 0.6698\n",
      "Epoch 79/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0868 - accuracy: 0.9126 - val_loss: 2.2492 - val_accuracy: 0.6696\n",
      "Epoch 80/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0873 - accuracy: 0.9127 - val_loss: 2.2517 - val_accuracy: 0.6681\n",
      "Epoch 81/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0868 - accuracy: 0.9121 - val_loss: 2.2482 - val_accuracy: 0.6704\n",
      "Epoch 82/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0861 - accuracy: 0.9112 - val_loss: 2.2480 - val_accuracy: 0.6696\n",
      "Epoch 83/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0860 - accuracy: 0.9116 - val_loss: 2.2482 - val_accuracy: 0.6699\n",
      "Epoch 84/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0854 - accuracy: 0.9130 - val_loss: 2.2492 - val_accuracy: 0.6696\n",
      "Epoch 85/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0850 - accuracy: 0.9116 - val_loss: 2.2492 - val_accuracy: 0.6696\n",
      "Epoch 86/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0854 - accuracy: 0.9112 - val_loss: 2.2445 - val_accuracy: 0.6699\n",
      "Epoch 87/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0852 - accuracy: 0.9119 - val_loss: 2.2455 - val_accuracy: 0.6694\n",
      "Epoch 88/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0845 - accuracy: 0.9122 - val_loss: 2.2476 - val_accuracy: 0.6699\n",
      "Epoch 89/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0844 - accuracy: 0.9131 - val_loss: 2.2456 - val_accuracy: 0.6704\n",
      "Epoch 90/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0849 - accuracy: 0.9117 - val_loss: 2.2457 - val_accuracy: 0.6701\n",
      "Epoch 91/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0840 - accuracy: 0.9127 - val_loss: 2.2443 - val_accuracy: 0.6698\n",
      "Epoch 92/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0841 - accuracy: 0.9117 - val_loss: 2.2468 - val_accuracy: 0.6703\n",
      "Epoch 93/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0838 - accuracy: 0.9124 - val_loss: 2.2442 - val_accuracy: 0.6699\n",
      "Epoch 94/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0829 - accuracy: 0.9123 - val_loss: 2.2473 - val_accuracy: 0.6698\n",
      "Epoch 95/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0828 - accuracy: 0.9131 - val_loss: 2.2453 - val_accuracy: 0.6699\n",
      "Epoch 96/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0832 - accuracy: 0.9125 - val_loss: 2.2455 - val_accuracy: 0.6698\n",
      "Epoch 97/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0830 - accuracy: 0.9127 - val_loss: 2.2484 - val_accuracy: 0.6696\n",
      "Epoch 98/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0829 - accuracy: 0.9117 - val_loss: 2.2470 - val_accuracy: 0.6698\n",
      "Epoch 99/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0822 - accuracy: 0.9117 - val_loss: 2.2489 - val_accuracy: 0.6691\n",
      "Epoch 100/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0822 - accuracy: 0.9116 - val_loss: 2.2483 - val_accuracy: 0.6698\n",
      "Epoch 101/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0817 - accuracy: 0.9128 - val_loss: 2.2479 - val_accuracy: 0.6699\n",
      "Epoch 102/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0814 - accuracy: 0.9129 - val_loss: 2.2464 - val_accuracy: 0.6699\n",
      "Epoch 103/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0815 - accuracy: 0.9126 - val_loss: 2.2464 - val_accuracy: 0.6701\n",
      "Epoch 104/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0824 - accuracy: 0.9119 - val_loss: 2.2486 - val_accuracy: 0.6694\n",
      "Epoch 105/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0812 - accuracy: 0.9130 - val_loss: 2.2454 - val_accuracy: 0.6696\n",
      "Epoch 106/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0812 - accuracy: 0.9128 - val_loss: 2.2469 - val_accuracy: 0.6698\n",
      "Epoch 107/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0813 - accuracy: 0.9127 - val_loss: 2.2442 - val_accuracy: 0.6703\n",
      "Epoch 108/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0808 - accuracy: 0.9131 - val_loss: 2.2458 - val_accuracy: 0.6701\n",
      "Epoch 109/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0809 - accuracy: 0.9124 - val_loss: 2.2458 - val_accuracy: 0.6698\n",
      "Epoch 110/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0813 - accuracy: 0.9116 - val_loss: 2.2440 - val_accuracy: 0.6699\n",
      "Epoch 111/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0802 - accuracy: 0.9116 - val_loss: 2.2431 - val_accuracy: 0.6704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0804 - accuracy: 0.9117 - val_loss: 2.2477 - val_accuracy: 0.6698\n",
      "Epoch 113/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0802 - accuracy: 0.9135 - val_loss: 2.2457 - val_accuracy: 0.6703\n",
      "Epoch 114/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0801 - accuracy: 0.9115 - val_loss: 2.2427 - val_accuracy: 0.6703\n",
      "Epoch 115/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0796 - accuracy: 0.9127 - val_loss: 2.2430 - val_accuracy: 0.6701\n",
      "Epoch 116/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0796 - accuracy: 0.9117 - val_loss: 2.2465 - val_accuracy: 0.6704\n",
      "Epoch 117/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0795 - accuracy: 0.9138 - val_loss: 2.2444 - val_accuracy: 0.6706\n",
      "Epoch 118/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0797 - accuracy: 0.9125 - val_loss: 2.2433 - val_accuracy: 0.6699\n",
      "Epoch 119/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0795 - accuracy: 0.9135 - val_loss: 2.2436 - val_accuracy: 0.6699\n",
      "Epoch 120/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0796 - accuracy: 0.9120 - val_loss: 2.2452 - val_accuracy: 0.6701\n",
      "Epoch 121/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0794 - accuracy: 0.9126 - val_loss: 2.2436 - val_accuracy: 0.6699\n",
      "Epoch 122/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0790 - accuracy: 0.9125 - val_loss: 2.2444 - val_accuracy: 0.6694\n",
      "Epoch 123/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0786 - accuracy: 0.9119 - val_loss: 2.2417 - val_accuracy: 0.6701\n",
      "Epoch 124/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0776 - accuracy: 0.9142 - val_loss: 2.2444 - val_accuracy: 0.6699\n",
      "Epoch 125/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0784 - accuracy: 0.9120 - val_loss: 2.2461 - val_accuracy: 0.6703\n",
      "Epoch 126/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0791 - accuracy: 0.9113 - val_loss: 2.2456 - val_accuracy: 0.6701\n",
      "Epoch 127/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0784 - accuracy: 0.9128 - val_loss: 2.2434 - val_accuracy: 0.6704\n",
      "Epoch 128/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0786 - accuracy: 0.9115 - val_loss: 2.2441 - val_accuracy: 0.6701\n",
      "Epoch 129/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0775 - accuracy: 0.9124 - val_loss: 2.2422 - val_accuracy: 0.6701\n",
      "Epoch 130/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0779 - accuracy: 0.9137 - val_loss: 2.2429 - val_accuracy: 0.6699\n",
      "Epoch 131/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0774 - accuracy: 0.9125 - val_loss: 2.2441 - val_accuracy: 0.6701\n",
      "Epoch 132/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0771 - accuracy: 0.9130 - val_loss: 2.2442 - val_accuracy: 0.6703\n",
      "Epoch 133/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0771 - accuracy: 0.9130 - val_loss: 2.2453 - val_accuracy: 0.6701\n",
      "Epoch 134/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0778 - accuracy: 0.9119 - val_loss: 2.2424 - val_accuracy: 0.6704\n",
      "Epoch 135/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0769 - accuracy: 0.9127 - val_loss: 2.2370 - val_accuracy: 0.6699\n",
      "Epoch 136/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0773 - accuracy: 0.9110 - val_loss: 2.2428 - val_accuracy: 0.6703\n",
      "Epoch 137/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0776 - accuracy: 0.9117 - val_loss: 2.2419 - val_accuracy: 0.6699\n",
      "Epoch 138/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0771 - accuracy: 0.9111 - val_loss: 2.2445 - val_accuracy: 0.6703\n",
      "Epoch 139/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0769 - accuracy: 0.9120 - val_loss: 2.2435 - val_accuracy: 0.6704\n",
      "Epoch 140/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0761 - accuracy: 0.9117 - val_loss: 2.2423 - val_accuracy: 0.6703\n",
      "Epoch 141/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0761 - accuracy: 0.9125 - val_loss: 2.2389 - val_accuracy: 0.6704\n",
      "Epoch 142/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0761 - accuracy: 0.9134 - val_loss: 2.2418 - val_accuracy: 0.6703\n",
      "Epoch 143/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0766 - accuracy: 0.9117 - val_loss: 2.2445 - val_accuracy: 0.6706\n",
      "Epoch 144/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0759 - accuracy: 0.9117 - val_loss: 2.2352 - val_accuracy: 0.6701\n",
      "Epoch 145/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0760 - accuracy: 0.9127 - val_loss: 2.2428 - val_accuracy: 0.6706\n",
      "Epoch 146/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0758 - accuracy: 0.9129 - val_loss: 2.2426 - val_accuracy: 0.6699\n",
      "Epoch 147/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0761 - accuracy: 0.9110 - val_loss: 2.2389 - val_accuracy: 0.6699\n",
      "Epoch 148/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0752 - accuracy: 0.9136 - val_loss: 2.2407 - val_accuracy: 0.6704\n",
      "Epoch 149/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0752 - accuracy: 0.9128 - val_loss: 2.2385 - val_accuracy: 0.6704\n",
      "Epoch 150/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0754 - accuracy: 0.9133 - val_loss: 2.2315 - val_accuracy: 0.6701\n",
      "Epoch 151/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0754 - accuracy: 0.9109 - val_loss: 2.2363 - val_accuracy: 0.6704\n",
      "Epoch 152/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0747 - accuracy: 0.9130 - val_loss: 2.2425 - val_accuracy: 0.6701\n",
      "Epoch 153/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0761 - accuracy: 0.9113 - val_loss: 2.2366 - val_accuracy: 0.6708\n",
      "Epoch 154/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0748 - accuracy: 0.9119 - val_loss: 2.2351 - val_accuracy: 0.6703\n",
      "Epoch 155/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0747 - accuracy: 0.9119 - val_loss: 2.2375 - val_accuracy: 0.6706\n",
      "Epoch 156/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0742 - accuracy: 0.9128 - val_loss: 2.2408 - val_accuracy: 0.6708\n",
      "Epoch 157/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0750 - accuracy: 0.9122 - val_loss: 2.2375 - val_accuracy: 0.6701\n",
      "Epoch 158/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0749 - accuracy: 0.9110 - val_loss: 2.2408 - val_accuracy: 0.6706\n",
      "Epoch 159/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0741 - accuracy: 0.9121 - val_loss: 2.2372 - val_accuracy: 0.6711\n",
      "Epoch 160/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0757 - accuracy: 0.9117 - val_loss: 2.2418 - val_accuracy: 0.6696\n",
      "Epoch 161/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0748 - accuracy: 0.9135 - val_loss: 2.2421 - val_accuracy: 0.6709\n",
      "Epoch 162/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0746 - accuracy: 0.9120 - val_loss: 2.2431 - val_accuracy: 0.6706\n",
      "Epoch 163/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0743 - accuracy: 0.9120 - val_loss: 2.2375 - val_accuracy: 0.6698\n",
      "Epoch 164/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0743 - accuracy: 0.9123 - val_loss: 2.2405 - val_accuracy: 0.6703\n",
      "Epoch 165/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0741 - accuracy: 0.9134 - val_loss: 2.2425 - val_accuracy: 0.6699\n",
      "Epoch 166/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0741 - accuracy: 0.9131 - val_loss: 2.2324 - val_accuracy: 0.6704\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0733 - accuracy: 0.9137 - val_loss: 2.2357 - val_accuracy: 0.6701\n",
      "Epoch 168/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0735 - accuracy: 0.9121 - val_loss: 2.2385 - val_accuracy: 0.6703\n",
      "Epoch 169/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0736 - accuracy: 0.9134 - val_loss: 2.2331 - val_accuracy: 0.6701\n",
      "Epoch 170/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0727 - accuracy: 0.9126 - val_loss: 2.2397 - val_accuracy: 0.6706\n",
      "Epoch 171/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0731 - accuracy: 0.9120 - val_loss: 2.2360 - val_accuracy: 0.6699\n",
      "Epoch 172/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0728 - accuracy: 0.9121 - val_loss: 2.2368 - val_accuracy: 0.6706\n",
      "Epoch 173/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0729 - accuracy: 0.9123 - val_loss: 2.2379 - val_accuracy: 0.6708\n",
      "Epoch 174/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0735 - accuracy: 0.9117 - val_loss: 2.2412 - val_accuracy: 0.6703\n",
      "Epoch 175/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0732 - accuracy: 0.9123 - val_loss: 2.2280 - val_accuracy: 0.6704\n",
      "Epoch 176/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0733 - accuracy: 0.9134 - val_loss: 2.2362 - val_accuracy: 0.6703\n",
      "Epoch 177/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0730 - accuracy: 0.9135 - val_loss: 2.2346 - val_accuracy: 0.6704\n",
      "Epoch 178/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0729 - accuracy: 0.9125 - val_loss: 2.2380 - val_accuracy: 0.6701\n",
      "Epoch 179/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0730 - accuracy: 0.9133 - val_loss: 2.2405 - val_accuracy: 0.6704\n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0721 - accuracy: 0.9136 - val_loss: 2.2407 - val_accuracy: 0.6701\n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0732 - accuracy: 0.9113 - val_loss: 2.2365 - val_accuracy: 0.6701\n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0722 - accuracy: 0.9125 - val_loss: 2.2368 - val_accuracy: 0.6701\n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0727 - accuracy: 0.9114 - val_loss: 2.2330 - val_accuracy: 0.6701\n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0719 - accuracy: 0.9138 - val_loss: 2.2344 - val_accuracy: 0.6706\n",
      "Epoch 185/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0723 - accuracy: 0.9125 - val_loss: 2.2358 - val_accuracy: 0.6704\n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0721 - accuracy: 0.9125 - val_loss: 2.2344 - val_accuracy: 0.6703\n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0722 - accuracy: 0.9126 - val_loss: 2.2382 - val_accuracy: 0.6704\n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0726 - accuracy: 0.9128 - val_loss: 2.2391 - val_accuracy: 0.6708\n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0723 - accuracy: 0.9114 - val_loss: 2.2344 - val_accuracy: 0.6706\n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0720 - accuracy: 0.9125 - val_loss: 2.2298 - val_accuracy: 0.6704\n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0716 - accuracy: 0.9130 - val_loss: 2.2407 - val_accuracy: 0.6704\n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0722 - accuracy: 0.9117 - val_loss: 2.2330 - val_accuracy: 0.6704\n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0717 - accuracy: 0.9125 - val_loss: 2.2376 - val_accuracy: 0.6708\n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0716 - accuracy: 0.9117 - val_loss: 2.2349 - val_accuracy: 0.6703\n",
      "Epoch 195/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0717 - accuracy: 0.9117 - val_loss: 2.2354 - val_accuracy: 0.6706\n",
      "Epoch 196/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0714 - accuracy: 0.9128 - val_loss: 2.2366 - val_accuracy: 0.6704\n",
      "Epoch 197/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0715 - accuracy: 0.9118 - val_loss: 2.2381 - val_accuracy: 0.6704\n",
      "Epoch 198/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0718 - accuracy: 0.9117 - val_loss: 2.2278 - val_accuracy: 0.6701\n",
      "Epoch 199/200\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.0709 - accuracy: 0.9128 - val_loss: 2.2367 - val_accuracy: 0.6708\n",
      "Epoch 200/200\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0712 - accuracy: 0.9125 - val_loss: 2.2398 - val_accuracy: 0.6706\n",
      "calculation time:  2103.0829956531525\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train the model\n",
    "start = time.time()\n",
    "model_history = model.fit([np.array(encode_input), np.array(decode_input)], \n",
    "                          np.array(decode_output),\n",
    "                          epochs=epochs,\n",
    "                          validation_split=0.2)\n",
    "end = time.time()\n",
    "print (\"calculation time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_accuracy(history):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    t = f.suptitle('Transformer Performance', fontsize=12)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    epoch_lenght = len(history.history['accuracy'])+1\n",
    "    epoch_list = list(range(1,epoch_lenght))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_xticks(np.arange(0, epoch_lenght, 20))\n",
    "    ax1.set_ylabel('Accuracy Value')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title('Accuracy')\n",
    "    l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "    # Plot loss\n",
    "    ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "    ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_xticks(np.arange(0, epoch_lenght, 20))\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('Loss')\n",
    "    l2 = ax2.legend(loc=\"best\")\n",
    "plot_accuracy(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_transformer import decode\n",
    "\n",
    "decoded_predicted = decode(\n",
    "    model,\n",
    "    encode_input,\n",
    "    start_token=target_token_dict['<START>'],\n",
    "    end_token=target_token_dict['<END>'],\n",
    "    pad_token=target_token_dict['<PAD>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beautiful girl with black hair was in the park. \n",
      "The climate of Canada is cooler than that of Japan. \n",
      "The climate of Canada is cooler than that of Japan. \n",
      "The committee consists of scientists and engineers. \n",
      "The couple carved their initials into the oak tree. \n",
      "The day will soon come when man can travel to Mars. \n",
      "The day will soon come when man can travel to Mars. \n",
      "The desire he has had for years has been fulfilled. \n",
      "The detective shadowed the suspect for four blocks. 4\n",
      "The doctors told Tom that Mary would never recover. \n"
     ]
    }
   ],
   "source": [
    "start_row = 5500\n",
    "for i in range(10):\n",
    "    print (source_input[start_row+i], \n",
    "           # target_tokens[start_row+i],\n",
    "           target_token_dict_inv[decoded_predicted[start_row+i][1:][0]])\n",
    "           # target_token_dict_inv[decoded_beam[start_row+i][1:][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a translated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word(input_sentence):\n",
    "    input_index = source_input.index(input_sentence)\n",
    "    input_text = source_input[input_index]\n",
    "    decoded_sentence = target_token_dict_inv[decoded_predicted[input_index][1:][0]]\n",
    "    print (input_text, decoded_sentence)\n",
    "    \n",
    "def translate_word_test(input_sentence):\n",
    "    decoded_predicted = decode(\n",
    "        model,\n",
    "        encode_input,\n",
    "        start_token=target_token_dict['<START>'],\n",
    "        end_token=target_token_dict['<END>'],\n",
    "        pad_token=target_token_dict['<PAD>'],\n",
    "    )\n",
    "    decoded_sentence = target_token_dict_inv[decoded_predicted[input_index][1:][0]]\n",
    "    print (decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jump! \n"
     ]
    }
   ],
   "source": [
    "translate_word(\"Jump!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run. \n"
     ]
    }
   ],
   "source": [
    "translate_word(\"Run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close your books. \n"
     ]
    }
   ],
   "source": [
    "translate_word(\"Close your books.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come and help me. \n"
     ]
    }
   ],
   "source": [
    "translate_word(\"Come and help me.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is too short to worry about things like that. \n"
     ]
    }
   ],
   "source": [
    "translate_word(\"Life is too short to worry about things like that.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom said that it probably wouldn't take too much time. \n"
     ]
    }
   ],
   "source": [
    "translate_word(\"Tom said that it probably wouldn't take too much time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
